self := import("@platforma-sdk/workflow-tengo:tpl")
assets := import("@platforma-sdk/workflow-tengo:assets")
maps := import("@platforma-sdk/workflow-tengo:maps")
exec := import("@platforma-sdk/workflow-tengo:exec")
xsv := import("@platforma-sdk/workflow-tengo:pframes.xsv")
json := import("json")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
soiExport := import(":soi-export")

self.defineOutputs("nodesResult", "treesResult")

// import MiXCR as a software to use
mitoolSw := assets.importSoftware("@platforma-open/milaboratories.software-mitool:main")
paggregateSw := assets.importSoftware("@platforma-open/milaboratories.software-paggregate:main")

inferPartitionKeyLength := func(data) {
	rType := data.info().Type.Name
	if (rType == pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED.Name) {
		return data.getDataAsJson().partitionKeyLength
    } else if (rType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED.Name) {
		return data.getDataAsJson().partitionKeyLength
	} else if (rType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED.Name) {
		superPartitionKeyLength := data.getDataAsJson().superPartitionKeyLength
		partitionKeyLength := data.getDataAsJson().partitionKeyLength
		return superPartitionKeyLength + partitionKeyLength
	} else if (rType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED.Name) {
		superPartitionKeyLength := data.getDataAsJson().superPartitionKeyLength
		partitionKeyLength := data.getDataAsJson().partitionKeyLength
		return superPartitionKeyLength + partitionKeyLength
	}
	return 0
}

self.body(func(inputs) {
	querySpec := inputs.querySpec
	queryData := inputs.queryData
	db := inputs.db

	dbData := "sequence\tname\n"
	for entry in db.sequences {
		dbData = dbData + entry.sequence + "\t" + entry.name + "\n"
	}

	originalAxesSpecs := querySpec.axesSpec
	modifiedAxesSpecs := []
	importAxesSpec := []

	aggregationGroupByTargets := []
	aggregationImportAxesSpec := []

	for idx, spec in originalAxesSpecs {
		colName := "key" + idx
		modifiedAxesSpecs = append(modifiedAxesSpecs, maps.deepMerge(
			spec,
			{ annotations: { "pl7.app/label": colName } }
		))
		importAxesSpec = append(importAxesSpec, {
			"column": colName,
			"spec": spec
		})

		// aggregating clonal and subtree axes away
		if spec.name != "pl7.app/dendrogram/subtreeId" && spec.name != "pl7.app/dendrogram/nodeId" {
			aggregationGroupByTargets = append(aggregationGroupByTargets, colName)
			aggregationImportAxesSpec = append(aggregationImportAxesSpec, {
				"column": colName,
				"spec": spec
			})
		}
	}
	modifiedQuerySpec := maps.deepMerge(querySpec, {
		axesSpec: modifiedAxesSpecs,
		annotations: {"pl7.app/label": "query"} })

	inputTsv := xsv.exportFrame([{spec: modifiedQuerySpec, data: queryData}], "tsv", {})

	searchCmd := exec.builder().
        printErrStreamToStdout().
        secret("MI_LICENSE", "MI_LICENSE").
        software(mitoolSw).
        arg("search").
        arg("--alphabet").arg(db.parameters.type).
		arg("--database").arg("database.tsv").
		writeFile("database.tsv", dbData).
		arg("--parameters").arg("params.json").
		writeFile("params.json", json.encode(db.parameters.searchParameters)).
		arg("--hits-only").
		arg("--target-column").arg("query").
        arg("input.tsv").addFile("input.tsv", inputTsv).
		arg("output.tsv").saveFile("output.tsv").
		run()

	resultCsv := searchCmd.getFile("output.tsv")

	resultColumns := soiExport.soiResultImportColumns(db.parameters)
	resultConvParams := {
		"axes": importAxesSpec,
		"columns": resultColumns,
		"storageFormat": "Binary",
		"partitionKeyLength": 0 // inferPartitionKeyLength(queryData)
	}

	aggregatedConvParams := {
		"axes": aggregationImportAxesSpec,
		"columns": resultColumns,
		"storageFormat": "Binary",
		"partitionKeyLength": 0 // inferPartitionKeyLength(queryData)
	}

	aggregations := []
	for col in resultColumns {
		aggregations = append(aggregations, {
			type: "first",
			src: col.column,
			dst: col.column
		})
	}

	aggregationWorkflow := { steps: [ {
			type: "aggregate",
			groupBy: aggregationGroupByTargets,
			aggregations: aggregations
		} ] }

	aggregateCmd := exec.builder().
        printErrStreamToStdout().
        software(paggregateSw).
        arg("--workflow").arg("wf.json").
		writeFile("wf.json", json.encode(aggregationWorkflow)).
		arg("input.tsv").addFile("input.tsv", resultCsv).
		arg("output.tsv").saveFile("output.tsv").
		run()

	aggregatedCsv := aggregateCmd.getFile("output.tsv")

	nodesResult := xsv.importFile(
        resultCsv,
        "tsv",
        resultConvParams
    )

	treesResult := xsv.importFile(
        aggregatedCsv,
        "tsv",
        aggregatedConvParams
    )

	return {
		nodesResult: nodesResult,
		treesResult: treesResult
	}
})
