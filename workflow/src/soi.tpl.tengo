self := import("@platforma-sdk/workflow-tengo:tpl")
ll := import("@platforma-sdk/workflow-tengo:ll")
assets := import("@platforma-sdk/workflow-tengo:assets")
maps := import("@platforma-sdk/workflow-tengo:maps")
exec := import("@platforma-sdk/workflow-tengo:exec")
xsv := import("@platforma-sdk/workflow-tengo:pframes.xsv")
json := import("json")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
soiExport := import(":soi-export")

self.defineOutputs("nodesResult", "treesResult")

// import MiXCR as a software to use
mitoolSw := assets.importSoftware("@platforma-open/milaboratories.software-mitool:main")
paggregateSw := assets.importSoftware("@platforma-open/milaboratories.software-ptransform:main")

inferPartitionKeyLength := func(data) {
	rType := data.info().Type.Name
	if (rType == pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED.Name) {
		return data.getDataAsJson().partitionKeyLength
    } else if (rType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED.Name) {
		return data.getDataAsJson().partitionKeyLength
	} else if (rType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED.Name) {
		superPartitionKeyLength := data.getDataAsJson().superPartitionKeyLength
		partitionKeyLength := data.getDataAsJson().partitionKeyLength
		return superPartitionKeyLength + partitionKeyLength
	} else if (rType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED.Name) {
		superPartitionKeyLength := data.getDataAsJson().superPartitionKeyLength
		partitionKeyLength := data.getDataAsJson().partitionKeyLength
		return superPartitionKeyLength + partitionKeyLength
	}
	return 0
}

self.body(func(inputs) {
	querySpec := inputs.querySpec
	queryData := inputs.queryData
	db := inputs.db

	dbData := "sequence\tname\n"
	for entry in db.sequences {
		dbData = dbData + entry.sequence + "\t" + entry.name + "\n"
	}

	originalAxesSpecs := querySpec.axesSpec
	modifiedAxesSpecs := []
	importAxesSpec := []

	aggregationGroupByTargets := []
	aggregationImportAxesSpec := []

	for idx, spec in originalAxesSpecs {
		colName := "key" + idx
		modifiedAxesSpecs = append(modifiedAxesSpecs, maps.deepMerge(
			spec,
			{ annotations: { "pl7.app/label": colName } }
		))
		importAxesSpec = append(importAxesSpec, {
			column: colName,
			spec: spec
		})

		// aggregating clonal and subtree axes away
		if spec.name != "pl7.app/dendrogram/subtreeId" && spec.name != "pl7.app/dendrogram/nodeId" {
			aggregationGroupByTargets = append(aggregationGroupByTargets, colName)
			aggregationImportAxesSpec = append(aggregationImportAxesSpec, {
				column: colName,
				spec: spec
			})
		}
	}
	modifiedQuerySpec := maps.deepMerge(querySpec, {
		axesSpec: modifiedAxesSpecs,
		annotations: {"pl7.app/label": "query"} })

	inputTsv := xsv.exportFrame([{spec: modifiedQuerySpec, data: queryData}], "tsv", {})

	actualSearchParameters := db.parameters.searchParameters
	if actualSearchParameters.type == "preset_alignment_search_top" {
		// nVDJRegion: 369
		// nCDR3: 51
		// aaVDJRegion: 641
		// aaCDR3: 93
		if db.parameters.type == "nucleotide" {
			actualSearchParameters = {
				type: "banded_alignment_search_top",
				scoring: {
					match: 1,
					mismatch: -2,
					gap: {
						type: "affine",
						openPenalty: -3,
						extensionPenalty: -1
					}
				},
				band: 9
			}
			if db.parameters.targetFeature == "CDR3" {
				actualSearchParameters.maxPenalty = 1 + int(51 * db.parameters.searchParameters.dissimilarityPercent / 100)
			} else if db.parameters.targetFeature == "VDJRegion" {
				actualSearchParameters.maxPenalty = 1 + int(369 * db.parameters.searchParameters.dissimilarityPercent / 100)
			} else {
				ll.panic("Unknown target feature: " + db.parameters.targetFeature)
			}
		} else if db.parameters.type == "amino-acid" {
			actualSearchParameters = {
				type: "banded_alignment_search_top",
				scoring: {
					matrix: "BLOSUM62",
					gap: {
						type: "affine",
						openPenalty: -10,
						extensionPenalty: -1
					}
				},
				band: 4
			}
			if db.parameters.targetFeature == "CDR3" {
				actualSearchParameters.maxPenalty = 1 + int(93 * db.parameters.searchParameters.dissimilarityPercent / 100)
			} else if db.parameters.targetFeature == "VDJRegion" {
				actualSearchParameters.maxPenalty = 1 + int(641 * db.parameters.searchParameters.dissimilarityPercent / 100)
			} else {
				ll.panic("Unknown target feature: " + db.parameters.targetFeature)
			}
		} else {
			ll.panic("Unknown alphabet: " + db.parameters.type)
		}
	}

	searchCmd := exec.builder().
        printErrStreamToStdout().
        secret("MI_LICENSE", "MI_LICENSE").
        software(mitoolSw).
        arg("search").
        arg("--alphabet").arg(db.parameters.type).
		arg("--database").arg("database.tsv").
		writeFile("database.tsv", dbData).
		arg("--parameters").arg("params.json").
		writeFile("params.json", json.encode(actualSearchParameters)).
		arg("--hits-only").
		arg("--target-column").arg("query").
        arg("input.tsv").addFile("input.tsv", inputTsv).
		arg("output.tsv").saveFile("output.tsv").
		run()

	resultCsv := searchCmd.getFile("output.tsv")

	resultColumns := soiExport.soiResultImportColumns(db.parameters)
	resultConvParams := {
		axes: importAxesSpec,
		columns: resultColumns,
		storageFormat: "Binary",
		partitionKeyLength: 0 // inferPartitionKeyLength(queryData)
	}

	aggregatedConvParams := {
		axes: aggregationImportAxesSpec,
		columns: resultColumns,
		storageFormat: "Binary",
		partitionKeyLength: 0 // inferPartitionKeyLength(queryData)
	}

	aggregations := []
	for col in resultColumns {
		aggregations = append(aggregations, {
			type: "first",
			src: col.column,
			dst: col.column
		})
	}

	aggregationWorkflow := { steps: [ {
			type: "aggregate",
			groupBy: aggregationGroupByTargets,
			aggregations: aggregations
		} ] }

	aggregateCmd := exec.builder().
        printErrStreamToStdout().
        software(paggregateSw).
        arg("--workflow").arg("wf.json").
		writeFile("wf.json", json.encode(aggregationWorkflow)).
		arg("input.tsv").addFile("input.tsv", resultCsv).
		arg("output.tsv").saveFile("output.tsv").
		run()

	aggregatedCsv := aggregateCmd.getFile("output.tsv")

	nodesResult := xsv.importFile(
        resultCsv,
        "tsv",
        resultConvParams
    )

	treesResult := xsv.importFile(
        aggregatedCsv,
        "tsv",
        aggregatedConvParams
    )

	return {
		nodesResult: nodesResult,
		treesResult: treesResult
	}
})
